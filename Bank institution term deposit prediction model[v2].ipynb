{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Bank institution term deposit prediction model[v2].ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6677wNfRz4Iv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f0886034-e942-476d-b983-8b423dfb9751"
      },
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import datetime as dt\n",
        "from matplotlib  import rcParams\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcF4WWA6z4JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "# rcParams[\"figure.figsize\"] = 16,6\n",
        "c = '#386B7F'"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX9K_Ogpz4JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upload the bank term deposit data set\n",
        "data = pd.read_csv(\"https://github.com/muluwork-shegaw/10Academy-week6/blob/master/data/bank-additional-full.csv?raw=true\",error_bad_lines=False,sep=';')\n",
        "# data_2 = pd.read_excel(\"bank-additional/bank-additional-full-1.csv\", low_memory=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwovMSbkz4JT",
        "colab_type": "text"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSIJp9Huz4JV",
        "colab_type": "text"
      },
      "source": [
        "#### Summery about the data\n",
        "\n",
        "      1) bank-additional-full.csv with all examples, ordered by date (from May 2008 to November 2010).\n",
        "      2) bank-additional.csv with 10% of the examples (4119), randomly selected from bank-additional-full.csv.\n",
        "   The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g., SVM).\n",
        "\n",
        "   The binary classification goal is to predict if the client will subscribe a bank term deposit (variable y).\n",
        "\n",
        "1. Number of Instances: 41188 for bank-additional-full.csv\n",
        "\n",
        "2. Number of Attributes: 20 + output attribute.\n",
        "3. **Missing Attribute Values: There are several missing values in some categorical attributes, all coded with the \"unknown\" label. These missing values can be treated as a possible class label or using deletion or imputation techniques.** \n",
        "4. Attribute information:\n",
        "\n",
        "\n",
        "   1. **bank client data:**\n",
        "   \n",
        "   1- age (numeric)\n",
        "   \n",
        "   2 - job : type of job (categorical: \"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\")\n",
        "   \n",
        "   3 - marital : marital status (categorical: \"divorced\",\"married\",\"single\",\"unknown\"; note: \"divorced\" means divorced or widowed)\n",
        "   4 - education (categorical: \"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\",\"unknown\")\n",
        "   \n",
        "   5 - default: has credit in default? (categorical: \"no\",\"yes\",\"unknown\")\n",
        "   \n",
        "   6 - housing: has housing loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
        "   \n",
        "   7 - loan: has personal loan? (categorical: \"no\",\"yes\",\"unknown\")\n",
        "   2. **related with the last contact of the current campaign:**\n",
        "   \n",
        "   8 - contact: contact communication type (categorical: \"cellular\",\"telephone\") \n",
        "   \n",
        "   9 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
        "   \n",
        "   10 - day_of_week: last contact day of the week (categorical: \"mon\",\"tue\",\"wed\",\"thu\",\"fri\")\n",
        "   \n",
        "   11 - duration: last contact duration, in seconds (numeric). Important note:  this attribute highly affects the output target (e.g., if duration=0 then y=\"no\"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
        "   3. **other attributes:**\n",
        "   \n",
        "   12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "  \n",
        "   13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
        "   \n",
        "   14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
        "   \n",
        "   15 - poutcome: outcome of the previous marketing campaign (categorical: \"failure\",\"nonexistent\",\"success\")\n",
        "   4. **social and economic context attributes**\n",
        "   \n",
        "   16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
        "   \n",
        "   17 - cons.price.idx: consumer price index - monthly indicator (numeric)    \n",
        "   \n",
        "   18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)  \n",
        "   \n",
        "   19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
        "   \n",
        "   20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
        "\n",
        "   5. Output variable (desired target):\n",
        "   \n",
        "   21 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4nBbr1Yz4JW",
        "colab_type": "text"
      },
      "source": [
        "### Data Glance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxDn7Ahhz4JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#glance the data\n",
        "print(\"shape of the  store data: \",data.shape)\n",
        "print(\"\\n unique no of each variable\\n\")\n",
        "print(data.nunique())\n",
        "print(\"\\n--------data summery------------\\n\")\n",
        "print(data.info())\n",
        "data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZTqDOXLz4Jt",
        "colab_type": "text"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2A5M4SVz4Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_col = data.select_dtypes(exclude=['int64', 'float64']).columns #catagorical columns\n",
        "non_cat_col = data.select_dtypes(include=['int64', 'float64']).columns # non catagorical variable\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GZ9avTkz4J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for  var in non_cat_col:\n",
        "    g = sns.FacetGrid(data)\n",
        "    g.map(sns.distplot, var,hist=True,kde=False)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJdocC5sz4KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def percentage_plot(data,col = cat_col):\n",
        "    total = float(len(data))\n",
        "    if type(col)==str:\n",
        "        plot = sns.countplot(col, data=data, order = data[col].value_counts().index)\n",
        "        for p in plot.patches:\n",
        "            percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
        "            x = p.get_x() + p.get_width()\n",
        "            y = p.get_height()\n",
        "            plot.annotate(percentage, (x, y),ha='center')\n",
        "\n",
        "    else :\n",
        "        fig, axs = plt.subplots(nrows=int(len(col)/2)+1, ncols=2, figsize=(20,40))\n",
        "        for i, var in enumerate(col):\n",
        "            row = i//2\n",
        "            pos = i % 2\n",
        "            plot = sns.countplot(var, data=data, ax=axs[row][pos],order = data[var].value_counts().index)\n",
        "            for p in plot.patches:\n",
        "                percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
        "                x = p.get_x() + p.get_width()\n",
        "                y = p.get_height()\n",
        "                plot.annotate(percentage, (x, y),ha='center')\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBl5OHdjz4KK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "percentage_plot(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wysyEcG_z4Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBSDZEUgz4Km",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*- when we see the day_of_week ,the have equa1 probable pecentage, which means the days has no that much effect.*\n",
        "\n",
        "**What is Imbalanced Data?**\n",
        "\n",
        "Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.\n",
        "\n",
        "For example, we may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n",
        "\n",
        "This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.\n",
        "\n",
        "we can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.\n",
        "\n",
        "\n",
        "**NB: As shown in figure above our target variable \"y\" 88.7 :11.3, there is class imbalance** \n",
        "\n",
        "soln:\n",
        "\n",
        "1.     rater than using accuracy use other metrics like confusion matrix,recall etc.\n",
        "2. Over-Sampling the minority class(yes class)\n",
        "3. modeling using XGBoost is better for the data given"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHS8vqlKz4Kn",
        "colab_type": "text"
      },
      "source": [
        "### Bivariate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Z3Fa9Rxpz4Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for  var in cat_col:\n",
        "    total_1 = float(len(data[data[\"y\"]==\"no\"]))\n",
        "    total_2 = float(len(data[data[\"y\"]==\"yes\"]))\n",
        "    total =[total_1,total_2]\n",
        "    i=0\n",
        "    \n",
        "\n",
        "    g = sns.catplot(x=var,kind='count', hue=\"y\",\n",
        "                col=\"y\", data=data,order = data[var].value_counts().index)\n",
        "    for ax in g.axes.ravel():\n",
        "            for p in ax.patches:\n",
        "                percentage = '{:.1f}%'.format(100 * p.get_height()/total[i])\n",
        "                x = p.get_x() + p.get_width()\n",
        "                y = p.get_height()\n",
        "                ax.annotate(percentage, (x, y),ha='center')\n",
        "            i =i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_n0eOmmTz4K5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "for  var in non_cat_col:\n",
        "    g = sns.FacetGrid(data,height=5)\n",
        "    g.map(sns.boxplot,y=data[var], x=data[\"y\"],hue=data[\"y\"],color=\"orange\",labels=[var])\n",
        "    g.add_legend()\n",
        "    plt.show()    \n",
        "    print(\"                    {}\\n\".format(var))\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yA9tWp9z4LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corr(data,col = None):\n",
        "    corr_matrix = data[col].corr()\n",
        "\n",
        "        \n",
        "    mask = np.zeros_like(corr_matrix, dtype = np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    f, ax = plt.subplots(figsize = (11, 9))\n",
        "\n",
        "    sns.heatmap(corr_matrix,annot=True,mask=mask, linewidths=.3,ax = ax)\n",
        "    plt.show()\n",
        "    return corr_matrix"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr8NIOb1z4LK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix=corr(data,non_cat_col)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVNCLU9Mz4LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[non_cat_col].plot(kind = 'box',showfliers = True,color = \"orange\") #with outliers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCgYQsPWz4Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[non_cat_col].plot(kind = 'box',showfliers = False)#without outliers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcYK264Zz4Lm",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdDTchq2z4Ln",
        "colab_type": "text"
      },
      "source": [
        "Since our dataset has many variables(20 features with out the target variable) and unknow value , we should carefull when we deal with outliers,missing value and dimentionality reduction since there is a class imbalance between \"yes\" and \"no\" in [y] variable as shown in figure above, we may biase  our model unless we consider such things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiBjlwE_z4Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ctM79ziz4L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "# from keras.models import Model\n",
        "# from Keras.layers  import Input,Dense\n",
        "# from keras import regularizers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import time\n",
        "time_start = time.time()\n",
        "\n",
        "\n",
        "class PreProcessor:\n",
        "\n",
        "    def __init__(self,data,target=None,drop_col=None,outlier_col=None,reduction_model=None,dim=None):\n",
        "        self.data = data\n",
        "        self.orig_data = data\n",
        "        self.target = target\n",
        "        self.drop_col =drop_col\n",
        "        self.outlier_col =outlier_col\n",
        "        \n",
        "        self.reduction_model = reduction_model\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.y_pred = None\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    '''\n",
        "       Missing Value Ratio\n",
        "       calculate the missing value ration\n",
        "       and drop that variable if it's missing value ratio \n",
        "       is greater than 20% and otherwise impute or drop that\n",
        "       value\n",
        "\n",
        "    ''' \n",
        "\n",
        "    def treat_missing(self,data):#feature selection based on missing value ratio\n",
        "        cat_col = self.data.select_dtypes(exclude=['int64', 'float64']).columns #catagorical columns\n",
        "        non_cat_col = self.data.select_dtypes(include=['int64', 'float64']).columns # non catagorical variable\n",
        "    \n",
        "        data[cat_col] = data[cat_col].replace(['unknown', 0], np.nan)\n",
        "        missing_value = data.isnull().sum()/len(data)*100\n",
        "        missing_var = []\n",
        "        for i,col in enumerate(self.data.columns):\n",
        "            if missing_value[i]>=20:   #setting the threshold as 20%\n",
        "                missing_var.append(col)\n",
        "                data.drop(col, axis=1,inplace=True)\n",
        "        print(\"\\n {} variables have dropped based on missing value ratio\".format(missing_var))\n",
        "          \n",
        "        '''\n",
        "            since the rest variable's missing value is \n",
        "            smaller we can impute them\n",
        "        '''\n",
        "        print(\"\\n {} missing values have been dropped\".format(data.isnull().sum().sum()))\n",
        "        data=data.fillna(data.mode().iloc[0])\n",
        "        \n",
        "        self.data = data\n",
        "        return data\n",
        "    '''\n",
        "        we may need to drop a columns which have a less variance and\n",
        "        more correlated variables, since if they are more correlated \n",
        "        they have same impact in indecating something and if they have least\n",
        "        variance which means they are almost constant values.\n",
        "    '''\n",
        "    def drope(self,data,col=None): # which is ongoing function,not implemented in optimized way yet.\n",
        "        data.drop(col,axis =1,inplace=True)\n",
        "        self.cat_col = data.select_dtypes(exclude=['int64', 'float64']).columns #catagorical columns\n",
        "        self.non_cat = data.select_dtypes(include=['int64', 'float64']).columns #catagorical columns\n",
        "        print(\"\\n {} variable has been dropped based on high correlation and less variance analysis\".format([col]))\n",
        "        self.data = data\n",
        "        return data\n",
        "\n",
        "    def deal_outliers(self,data,columns):#handle outliers based on interquartile range(iqr)\n",
        "        orig_data = data\n",
        "\n",
        "        print(\"\\n before dealing with the ouliers the shape of the data\",\n",
        "              data.shape)\n",
        "        for col in columns:\n",
        "            q1 = data[col].quantile(0.25) # quartile 1\n",
        "            q3 = data[col].quantile(0.75) # quartile 4\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 -(1.5 * iqr) #lower whishker\n",
        "            upper_bound = q3 +(1.5 * iqr) #upper whishker\n",
        "            data = data[data[col]>lower_bound] #remove which are greater than upper whishker\n",
        "            data = data[data[col]<upper_bound] #remove which are less than lower whishker\n",
        "\n",
        "        print(\"after removing the outliers\",data.shape)\n",
        "        \n",
        "        self.data = data\n",
        "        self.orig_data = orig_data\n",
        "        \n",
        "        print(\"{}  data has been removed based on outlier analysis\".format(len(self.orig_data)-len(self.data)))\n",
        "        return data # return data with outlier and without outlier\n",
        "    \n",
        "    def encoder(self,data):# encode the catagorical value\n",
        "        label_encoder = LabelEncoder()\n",
        "        binary_col = []\n",
        "        for col in (data.columns):\n",
        "            if data[col].nunique() == 2:\n",
        "                binary_col.append(col)\n",
        "                data[col] = label_encoder.fit_transform(data[col])# encode binary variables\n",
        "        print(\"\\n{} variables have been encoded based on label encoding the rest encoded as dummy variable\".format(binary_col))\n",
        "        \n",
        "        data = pd.get_dummies(data) #encode not binary variables\n",
        "        self.data = data\n",
        "        return data\n",
        "    \n",
        "    def scaler(self,data, scaler= MinMaxScaler()): # not implmented in optimized way yet\n",
        "        data[:] = scaler.fit_transform(data[:]) #scaleing the given data\n",
        "        self.data = data\n",
        "        print(\"\\n---- Scaling the data based on\",scaler)\n",
        "        return data\n",
        "    \n",
        "    def reduce_dimension(self,data,reduction_model = 'tsne',dim=3): # reduce the dimentions of a given data\n",
        "        target = self.target\n",
        "        data_1 = data\n",
        "        d = data_1.drop(target,axis=1)\n",
        "        \n",
        "        print(\"\\nThe data is reduced based on {0} algorithm to {1} dimentions\".format(reduction_model,dim+1))\n",
        "        print(\"Executing the reduction starts and wait ......\")\n",
        "        \n",
        "        if reduction_model =='pca': # using PCA\n",
        "            pca = PCA(n_components=dim)\n",
        "            reduced_df = pd.DataFrame(pca.fit_transform(d), # reduce the dimention and convert to data frame\n",
        "               columns=[f'pca {i}'  for i in range(1,dim+1)]) \n",
        "            print ('PCA done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "            \n",
        "        elif reduction_model =='tsne':\n",
        "            tsne = TSNE(n_components = dim, n_iter = 300)\n",
        "            reduced_df =pd.DataFrame(tsne.fit_transform(d),\n",
        "                        columns=[f'tsne {i}'  for i in range(1,dim+1)])\n",
        "            print ('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "    #     else:\n",
        "    #         input_dim = d.shape[1]\n",
        "    #         input_layer = Input(shape = (input_dim,))\n",
        "    #         encoder_layer_1 = Dense(40,activation='tanh',\n",
        "    #                         activity_regularizer=regularizers.l1(10e-5)(input))(input_layer)\n",
        "    #         encoder_layer_2 = Dense(30,activation='tanh')(encoder_layer_1)\n",
        "    #         encoder_layer_3 = Dense(encoding_dim,activation='tanh')(encoder_layer_2)\n",
        "    #         #create encoder model\n",
        "    #         encoder = Model(inputs = input_layer, outputs = encoder_layer_3)\n",
        "    #         reduced_df= pd.DataFrame(encoder.predict(d))\n",
        "                   \n",
        "    # reset the index to get clean data(without nan)              \n",
        "        \n",
        "        data_1[target].reset_index(drop=True, inplace=True)\n",
        "        reduced_df.reset_index(drop=True, inplace=True)\n",
        "        final_df = pd.concat([reduced_df,data_1[target]],axis=1)\n",
        "                   \n",
        "        self.data = final_df\n",
        "        return final_df\n",
        "    \n",
        "    def split_data(self,data,target):  #Split the data\n",
        "        \n",
        "        print(\"\\nspliting the data as train and test\")\n",
        "        X=data.drop(target,axis=1)\n",
        "        y=data[target]\n",
        "        from sklearn.datasets import make_classification\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify =y,random_state=1)#use stratified kfold distribution for imbalanced class distributin\n",
        "        return X_train, X_test, y_train, y_test\n",
        "                   \n",
        "    def pipe_and_filter(self):# this function is to pipe earlier steps\n",
        "        new = self.treat_missing(self.data)\n",
        "        new = self.drope(self.data,col= self.drop_col)\n",
        "        new = self.deal_outliers(self.data,columns = self.outlier_col)\n",
        "        new = self.encoder(self.data)\n",
        "        new_scaled = self.scaler(self.data)\n",
        "        new = self.reduce_dimension(self.data,reduction_model = self.reduction_model,dim=self.dim)\n",
        "        X_train, X_test, y_train, y_test =self.split_data(self.data,self.target)\n",
        "        print(\"DONE!\")\n",
        "        \n",
        "        result = [X_train, X_test, y_train, y_test]\n",
        "        df =[new_scaled, new]\n",
        "                   \n",
        "        return df,result\n",
        "        \n",
        "        \n",
        "            \n",
        "        \n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVzt34y9z4L-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_cat_col = data.select_dtypes(exclude=['int64', 'float64']).columns #catagorical columns\n",
        "corr_matrix = data.drop(new_cat_col, axis = 1).corr()\n",
        "co =corr_matrix.unstack().sort_values().drop_duplicates() \n",
        "\n",
        "print(\"variance of data\\n\\n\",data.var().sort_values())\n",
        "pd.DataFrame(co,columns=['correlation']).tail(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9vWDPbwz4MI",
        "colab_type": "text"
      },
      "source": [
        "- Since  emp.var.rate ,euribor3m & nr.employed are highly correlated and the variance of emp.var.rate and euribor3m is less variance  we can drop emp.var.rate and euribor3m \n",
        "- we can drop day_of week too since it has equal probable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiNuHAMuz4MJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outlier_col = ['age', 'campaign','cons.conf.idx']\n",
        "drop_col =  [\"emp.var.rate\",\"euribor3m\",'day_of_week',\"duration\"]\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIlRlDnuz4MR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "25930e1e-0ed1-4569-a15d-42ea1f399235"
      },
      "source": [
        "processor = PreProcessor(data,'y',drop_col,outlier_col,\"pca\",7)\n",
        "new_data,split_result = processor.pipe_and_filter()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " [] variables have dropped based on missing value ratio\n",
            "\n",
            " 4121 missing values have been dropped\n",
            "\n",
            " [['emp.var.rate', 'euribor3m', 'day_of_week', 'duration']] variable has been dropped based on high correlation and less variance analysis\n",
            "\n",
            " before dealing with the ouliers the shape of the data (41188, 16)\n",
            "after removing the outliers (36967, 16)\n",
            "4221  data has been removed based on outlier analysis\n",
            "\n",
            "['housing', 'loan', 'contact', 'y'] variables have been encoded based on label encoding the rest encoded as dummy variable\n",
            "\n",
            "---- Scaling the data based on MinMaxScaler(copy=True, feature_range=(0, 1))\n",
            "\n",
            "The data is reduced based on pca algorithm to 8 dimentions\n",
            "Executing the reduction starts and wait ......\n",
            "PCA done! Time elapsed: 20.35617685317993 seconds\n",
            "\n",
            "spliting the data as train and test\n",
            "DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbSNwRCUz4MZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for later use\n",
        "scaled_data = new_data[0]\n",
        "reduced_data =new_data[1]\n",
        "\n",
        "X_train =split_result[0]\n",
        "X_test = split_result[1]\n",
        "y_train =split_result[2]\n",
        "y_test = split_result[3]\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Uu1DOZepN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduced_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXbJreCaz4N6",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhi203Njz4N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "\n",
        "import scipy.stats as stat\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.metrics import *\n",
        "import datetime"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zvTVv6Cz4OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kr8McsVz4ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfZmSqrXz4OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassifierModeling:\n",
        "    def __init__(self,model_name,X_train,y_train,X_test,y_test,kfold):\n",
        "        \n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.kfold =kfold\n",
        "        self.y_pred = None\n",
        "        self.model_name = model_name\n",
        "        if self.model_name== \"RandomForestClassifier\":\n",
        "            self.model = RandomForestClassifier()\n",
        "        elif self.model_name == \"LogisticRegression\":\n",
        "            self.model = LogisticRegression(solver='liblinear', random_state=0)\n",
        "        elif self.model_name == \"DecisionTreeClassifier\":\n",
        "            self.model = DecisionTreeClassifier()\n",
        "        elif self.model_name == \"XG_Boost\":\n",
        "            data_dmatrix = xgb.DMatrix(data=self.X_train,label=self.y_train)\n",
        "            self.model = xgb.XGBClassifier()\n",
        "        elif self.model_name ==\"Multilayer Perceptron\":\n",
        "            print(\"not implemented yet\")\n",
        "        elif self.model_name == \"svm\" :\n",
        "            self.model = svm.SVC(kernel='linear', C=0.01)\n",
        "        elif self.model_name == \"adboost\":\n",
        "             self.model =AdaBoostClassifier()\n",
        "        elif self.model_name == \"gradienBoost\":\n",
        "             self.model = GradientBoostingClassifier()\n",
        "        \n",
        "    def fit(self):\n",
        "        self.model.fit(self.X_train,self.y_train)\n",
        "\n",
        "    def get_predicate(self): \n",
        "        self.y_pred = pd.Series(self.model.predict(self.X_test),name =\"predict\")\n",
        "        return self.y_pred \n",
        "    def get_MSE(self):\n",
        "        return mean_squared_error(self.y_test,self.y_pred)\n",
        "    def get_score(self):\n",
        "        return -(r2_score(self.y_test,self.y_pred))\n",
        "        \n",
        "    def get_loss(self):\n",
        "        return np.sqrt(mean_squared_error(self.y_test,self.y_pred))\n",
        "    def validate_model(self):\n",
        "        print\n",
        "        model_fit =pd.DataFrame()\n",
        "        model_fit = pd.concat([self.y_pred, self.y_test], axis=1)\n",
        "\n",
        "        \n",
        "\n",
        "        # matrix = confusion_matrix(self.y_test, self.y_pred)\n",
        "        # d= plot_confusion_matrix(self.model, self.X_test, self.y_test,\n",
        "        #                                      display_labels=[\"yes\",\"no\"],\n",
        "        #                                      cmap=plt.cm.Blues)\n",
        "\n",
        "        total = float(len(model_fit))\n",
        "        fig, axs = plt.subplots(1,3,squeeze=False,figsize=(15, 3))\n",
        "        plt.rcParams.update({'font.size': 10})\n",
        "        d= plot_confusion_matrix(self.model, self.X_test, self.y_test,\n",
        "                                             display_labels=[\"yes\",\"no\"],\n",
        "                                             cmap=plt.cm.Blues,ax=axs[0,2])\n",
        "        d.ax_.set_title(\"{} confusion matrix\".format(self.model_name))\n",
        "        \n",
        "\n",
        "\n",
        "        for ax in axs.flatten():\n",
        "            \n",
        "            for i, var in enumerate(model_fit.columns):\n",
        "                ax  = sns.countplot(var, data=model_fit, ax=axs[0][i])\n",
        "                ax .set_title(self.model_name)\n",
        "                for p in ax.patches:\n",
        "                    percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
        "                    x = p.get_x() + p.get_width()\n",
        "                    y = p.get_height()\n",
        "                    ax .annotate(percentage, (x, y),ha='center')\n",
        "        #fig.savefig(\"https://github.com/muluwork-shegaw/10Academy-week6/blob/master/data/{}.png\".format(self.model_name))\n",
        "     \n",
        "        return matrix,model_fit\n",
        "    def get_eff_model(self):\n",
        "        if self.model_name != \"svm\":\n",
        "        \n",
        "            metrics = pd.DataFrame()\n",
        "            metrics[\"model\"] = [self.model_name]\n",
        "            metrics[\"MSE\"] = mean_squared_error(self.y_test,self.y_pred)\n",
        "            metrics[\"Loss\"] = np.sqrt(mean_squared_error(self.y_test,self.y_pred))\n",
        "            metrics[\"Score\"] = -(r2_score(self.y_test,self.y_pred))\n",
        "            metrics[\"Kappa\"] = cohen_kappa_score(self.y_test, self.y_pred)\n",
        "            metrics[\"ROC_Auc\"] = roc_auc_score(self.y_test, self.y_pred)\n",
        "            metrics[\"precision\"] = precision_score(self.y_test, self.y_pred)\n",
        "            metrics[\"recall\"] = recall_score(self.y_test, self.y_pred)\n",
        "            metrics[\"f1_score\"] = f1_score(self.y_test, self.y_pred)\n",
        "            metrics[\"accuracy\"] = accuracy_score(self.y_test, self.y_pred)\n",
        "        \n",
        "\n",
        "        \n",
        "            return metrics\n",
        "    def get_accuracy_with_kfold(self):\n",
        "       \n",
        "        return cross_val_score(self.model,self.X_test, self.y_test,cv=self.kfold, scoring= 'accuracy').mean()\n",
        "        \n",
        "    def get_loss_with_kfold(self,valid_data,valid_targ,k_fold):\n",
        "        return -(cross_val_score(self.model,self.X_test, self.y_test,cv=self.kfold, scoring= 'neg_log_loss').mean())\n",
        "    def eff_model_with_kfold(self):\n",
        "        if self.model_name != \"svm\":\n",
        "\n",
        "            scoring = [\"accuracy\",\"neg_log_loss\",\"r2\",\n",
        "                 \"neg_mean_squared_error\",\"neg_mean_absolute_error\"] \n",
        "\n",
        "            metrics = pd.DataFrame()\n",
        "            metrics[\"model\"] = [self.model_name]\n",
        "            for scor in scoring:\n",
        "                score = []\n",
        "                result = cross_val_score(estimator= self.model, X=self.X_test, y= self.y_test,cv=self.kfold,scoring=scor )\n",
        "                score.append(result.mean())\n",
        "\n",
        "                metrics[scor] =pd.Series(score)\n",
        "\n",
        "            return metrics\n",
        "        \n",
        "    def get_feature_impo(self):\n",
        "        if self.model_name != \"LogisticRegression\":\n",
        "            feat_importance = pd.Series(self.model.feature_importances_, index=self.X_train.columns)\n",
        "            feat_importance.plot(kind='bar')\n",
        "            plt.show()\n",
        "        return feat_importance\n",
        "    def get_summary(self):# for feature importance of logistic regression\n",
        "        if self.model_name == \"LogisticRegression\":\n",
        "\n",
        "            denom = (2.0*(1.0+np.cosh(self.model.decision_function(X))))\n",
        "            denom = np.tile(denom,(X.shape[1],1)).T\n",
        "            F_ij = np.dot((X/denom).T,X) ## Fisher Information Matrix\n",
        "            Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
        "            sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
        "            z_scores = self.model.coef_[0]/sigma_estimates # z-score for eaach model coefficient\n",
        "            p_values = [stat.norm.sf(abs(x))*2 for x in z_scores] ### two tailed test for p-values\n",
        "\n",
        "            z_scores = z_scores\n",
        "            p_values = p_values\n",
        "            sigma_estimates = sigma_estimates\n",
        "            F_ij = F_ij\n",
        "\n",
        "            summary= pd.DataFrame()\n",
        "            summary[\"features\"] = self.X_train.columns\n",
        "            summary[\"z_score\"] = self.z_scores\n",
        "            summary[\"p_value\"] = self.p_values\n",
        "            sns.barplot(summary[\"features\"],summary[\"p_value\"], data=summary)\n",
        "        return summary\n",
        "\n",
        "    def save_model(self):\n",
        "        \n",
        "        now = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "        # Saving model to disk\n",
        "        filename = now + '.pkl'\n",
        "        pickle.dump(self.model, open(filename, 'wb'))\n",
        "        return filename\n",
        "    def make_it_stratified(self,data,model_name,target):\n",
        "        X=data.drop(target,axis=1)\n",
        "        y=data[target]\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99, 0.01], flip_y=0, random_state=1)\n",
        "\n",
        "\n",
        "        '''\n",
        "        use stratified k-fold cross-validation \n",
        "        with imbalanced datasets to preserve the class distribution in the train and test \n",
        "        sets for each evaluation of a given model.\n",
        "        '''\n",
        "        eff_kfold =[]\n",
        "        eff =[]\n",
        "        model_pred = []\n",
        "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        #enumerate the splits and summarize the distributions\n",
        "        for train_ix, test_ix in kfold.split(X, y):\n",
        "            # select rows\n",
        "            X_train, X_test = X[train_ix], X[test_ix]\n",
        "            y_train, y_test = y[train_ix], y[test_ix]\n",
        "\n",
        "            self.fit()\n",
        "            self.get_predicate()\n",
        "            matrix,model_fit=self.validate_model()\n",
        "            model_pred.append(model_fit)\n",
        "    \n",
        "            eff_kfold.append(self.eff_model_with_kfold())\n",
        "            eff.append(self.get_eff_model())    \n",
        "\n",
        "            # summarize train and test composition\n",
        "            train_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\n",
        "            test_0, test_1 = len(y_test[y_test==0]), len(y_test[y_test==1])\n",
        "            print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n",
        "        df_kfold = pd.concat(eff_kfold)\n",
        "        df_eff = pd.concat(eff)\n",
        "        return df_kfold,df_eff,model_pred\n",
        "            \n",
        "        "
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZT345xz4Ol",
        "colab_type": "text"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLRvoHryz4QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers =[\"RandomForestClassifier\",\"LogisticRegression\",\n",
        "              \"DecisionTreeClassifier\", \"XG_Boost\",\"svm\",\"adboost\",\"gradienBoost\"]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbR_vxYZ5Jah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eff_kfold =[]\n",
        "eff =[]\n",
        "mode = []\n",
        "for model_classifier in classifiers:\n",
        "    classifier = ClassifierModeling(model_classifier,X_train,y_train,X_test,y_test,k_fold)\n",
        "    classifier.fit()\n",
        "    pridict = classifier.get_predicate()\n",
        "    matrix,model_fit=classifier.validate_model()\n",
        "    mode.append(model_fit)\n",
        "    \n",
        "    eff_kfold.append(classifier.eff_model_with_kfold())\n",
        "    eff.append(classifier.get_eff_model())    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmh8czjs5tHU",
        "colab_type": "text"
      },
      "source": [
        "- **A confusion matrix** is a matrix (table) that can be used to measure the performance of an machine learning algorithm, usually a supervised learning one. Each row of the confusion matrix represents the instances of an actual class and each column represents the instances of a predicted class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_n9nRq05JND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "a88ab4b5-ec22-4f24-b28f-f0dfaaea61b4"
      },
      "source": [
        "c= y_test\n",
        "d= classifier.y_pred\n",
        "a = pd.concat([c,d],axis=1)\n",
        "\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3692</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3693</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3694</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3695</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3696</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3697 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y  predict\n",
              "0     0.0      0.0\n",
              "1     0.0      0.0\n",
              "2     0.0      0.0\n",
              "3     1.0      1.0\n",
              "4     0.0      0.0\n",
              "...   ...      ...\n",
              "3692  0.0      0.0\n",
              "3693  0.0      0.0\n",
              "3694  0.0      0.0\n",
              "3695  0.0      0.0\n",
              "3696  0.0      0.0\n",
              "\n",
              "[3697 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emrebux-5I5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "39c3116e-3648-4ada-90ed-93b330c8d25f"
      },
      "source": [
        "df = pd.concat(eff)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>MSE</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Score</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>ROC_Auc</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.121450</td>\n",
              "      <td>0.348497</td>\n",
              "      <td>0.237006</td>\n",
              "      <td>0.202595</td>\n",
              "      <td>0.578571</td>\n",
              "      <td>0.396985</td>\n",
              "      <td>0.193627</td>\n",
              "      <td>0.260297</td>\n",
              "      <td>0.878550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.110360</td>\n",
              "      <td>0.332204</td>\n",
              "      <td>0.124050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.889640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>0.150663</td>\n",
              "      <td>0.388153</td>\n",
              "      <td>0.534548</td>\n",
              "      <td>0.216751</td>\n",
              "      <td>0.606165</td>\n",
              "      <td>0.308483</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.301129</td>\n",
              "      <td>0.849337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XG_Boost</td>\n",
              "      <td>0.110360</td>\n",
              "      <td>0.332204</td>\n",
              "      <td>0.124050</td>\n",
              "      <td>0.003805</td>\n",
              "      <td>0.501073</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.004878</td>\n",
              "      <td>0.889640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>adboost</td>\n",
              "      <td>0.110901</td>\n",
              "      <td>0.333018</td>\n",
              "      <td>0.129560</td>\n",
              "      <td>-0.001078</td>\n",
              "      <td>0.499696</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.889099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gradienBoost</td>\n",
              "      <td>0.110360</td>\n",
              "      <td>0.332204</td>\n",
              "      <td>0.124050</td>\n",
              "      <td>0.018742</td>\n",
              "      <td>0.505367</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.012255</td>\n",
              "      <td>0.023923</td>\n",
              "      <td>0.889640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model       MSE      Loss  ...    recall  f1_score  accuracy\n",
              "0  RandomForestClassifier  0.121450  0.348497  ...  0.193627  0.260297  0.878550\n",
              "0      LogisticRegression  0.110360  0.332204  ...  0.000000  0.000000  0.889640\n",
              "0  DecisionTreeClassifier  0.150663  0.388153  ...  0.294118  0.301129  0.849337\n",
              "0                XG_Boost  0.110360  0.332204  ...  0.002451  0.004878  0.889640\n",
              "0                 adboost  0.110901  0.333018  ...  0.000000  0.000000  0.889099\n",
              "0            gradienBoost  0.110360  0.332204  ...  0.012255  0.023923  0.889640\n",
              "\n",
              "[6 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKDMGhIL5Jm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "a36739a1-6d1f-43e2-c985-adac98829500"
      },
      "source": [
        "df_kfold =pd.concat(eff_kfold)\n",
        "df_kfold"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>neg_log_loss</th>\n",
              "      <th>r2</th>\n",
              "      <th>neg_mean_squared_error</th>\n",
              "      <th>neg_mean_absolute_error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.872600</td>\n",
              "      <td>-0.589495</td>\n",
              "      <td>-0.330594</td>\n",
              "      <td>-0.130376</td>\n",
              "      <td>-0.129564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.889640</td>\n",
              "      <td>-0.321747</td>\n",
              "      <td>-0.124050</td>\n",
              "      <td>-0.110360</td>\n",
              "      <td>-0.110360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>0.812824</td>\n",
              "      <td>-6.372992</td>\n",
              "      <td>-0.911998</td>\n",
              "      <td>-0.186362</td>\n",
              "      <td>-0.187448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XG_Boost</td>\n",
              "      <td>0.889371</td>\n",
              "      <td>-0.324521</td>\n",
              "      <td>-0.126763</td>\n",
              "      <td>-0.110629</td>\n",
              "      <td>-0.110629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>adboost</td>\n",
              "      <td>0.888018</td>\n",
              "      <td>-0.667143</td>\n",
              "      <td>-0.140568</td>\n",
              "      <td>-0.111982</td>\n",
              "      <td>-0.111982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gradienBoost</td>\n",
              "      <td>0.888017</td>\n",
              "      <td>-0.326756</td>\n",
              "      <td>-0.140629</td>\n",
              "      <td>-0.111983</td>\n",
              "      <td>-0.111983</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model  ...  neg_mean_absolute_error\n",
              "0  RandomForestClassifier  ...                -0.129564\n",
              "0      LogisticRegression  ...                -0.110360\n",
              "0  DecisionTreeClassifier  ...                -0.187448\n",
              "0                XG_Boost  ...                -0.110629\n",
              "0                 adboost  ...                -0.111982\n",
              "0            gradienBoost  ...                -0.111983\n",
              "\n",
              "[6 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qjoZXNQ6JNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjQDHjt8cS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxvnOVJO_PvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.2\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model, models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from google.colab import files\n",
        "import time\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import random\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4o2bNWj5IoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # demonstration of calculating metrics for a neural network model using sklearn\n",
        "# from sklearn.datasets import make_circles\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import precision_score\n",
        "# from sklearn.metrics import recall_score\n",
        "# from sklearn.metrics import f1_score\n",
        "# from sklearn.metrics import cohen_kappa_score\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense\n",
        "\n",
        "# # generate and prepare the dataset\n",
        "# def get_data():\n",
        "\n",
        "# \treturn X_train, y_train, X_test, y_test\n",
        "\n",
        "# # define and fit the model\n",
        "# def get_model(trainX, trainy):\n",
        "# \t# define model\n",
        "# \tmodel = Sequential()\n",
        "# \tmodel.add(Dense(100, input_dim=2, activation='relu'))\n",
        "# \tmodel.add(Dense(1, activation='sigmoid'))\n",
        "# \t# compile model\n",
        "# \tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# \t# fit model\n",
        "# \tmodel.fit(trainX, trainy, epochs=300, verbose=0)\n",
        "# \treturn model\n",
        "\n",
        "# # generate data\n",
        "# trainX, trainy, testX, testy = get_data()\n",
        "# # fit model\n",
        "# model = get_model(trainX, trainy)\n",
        "\n",
        "\n",
        "# # predict probabilities for test set\n",
        "# yhat_probs = model.predict(testX, verbose=0)\n",
        "# # predict crisp classes for test set\n",
        "# yhat_classes = model.predict_classes(testX, verbose=0)\n",
        "# # reduce to 1d array\n",
        "# yhat_probs = yhat_probs[:, 0]\n",
        "# yhat_classes = yhat_classes[:, 0]\n",
        "\n",
        "# # accuracy: (tp + tn) / (p + n)\n",
        "# accuracy = accuracy_score(testy, yhat_classes)\n",
        "# print('Accuracy: %f' % accuracy)\n",
        "# # precision tp / (tp + fp)\n",
        "# precision = precision_score(testy, yhat_classes)\n",
        "# print('Precision: %f' % precision)\n",
        "# # recall: tp / (tp + fn)\n",
        "# recall = recall_score(testy, yhat_classes)\n",
        "# print('Recall: %f' % recall)\n",
        "# # f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(testy, yhat_classes)\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "# # kappa\n",
        "# kappa = cohen_kappa_score(testy, yhat_classes)\n",
        "# print('Cohens kappa: %f' % kappa)\n",
        "# # ROC AUC\n",
        "# auc = roc_auc_score(testy, yhat_probs)\n",
        "# print('ROC AUC: %f' % auc)\n",
        "# # confusion matrix\n",
        "# matrix = confusion_matrix(testy, yhat_classes)\n",
        "# print(matrix)\n",
        "# def eff_model(self):\n",
        "#     metrics = pd.DataFrame()\n",
        "#     metrics[\"model\"] = [\"Multilayer Perceptron Model\"]\n",
        "#     metrics[\"Kappa\"] = cohen_kappa_score(testy, yhat_classes)\n",
        "#     metrics[\"ROC_Auc\"] = roc_auc_score(testy, yhat_probs)\n",
        "#     metrics[\"precision\"] = precision_score(testy, yhat_classes)\n",
        "#     metrics[\"recall\"] = recall_score(testy, yhat_classes)\n",
        "#     metrics[\"f1_score\"] = f1_score(testy, yhat_classes)\n",
        "#     metrics[\"accuracy\"] = accuracy_score(testy, yhat_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNK6YA_6_NlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fwr0J496H49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}