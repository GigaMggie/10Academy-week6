{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled328.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muluwork-shegaw/10Academy-week6/blob/master/Data.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L30XvULyvCvv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f646e77a-999e-4749-b75a-6772ae73fe81"
      },
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import datetime as dt\n",
        "from matplotlib  import rcParams\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjYplBKbBc88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"https://github.com/muluwork-shegaw/10Academy-week6/blob/master/data/bank-additional-full.csv?raw=true\",error_bad_lines=False,sep=';')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RreW5RWNBmt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "# from keras.models import Model\n",
        "# from Keras.layers  import Input,Dense\n",
        "# from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import time\n",
        "time_start = time.time()\n",
        "\n",
        "\n",
        "class PreProcessor:\n",
        "\n",
        "    def __init__(self,data,target,drop_col,outlier_col,reduction_model,dim):\n",
        "        self.data = data\n",
        "        self.orig_data = data\n",
        "        self.target = target\n",
        "        self.drop_col =drop_col\n",
        "        self.outlier_col =outlier_col\n",
        "        \n",
        "        self.reduction_model = reduction_model\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.y_pred = None\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    '''\n",
        "       Missing Value Ratio\n",
        "       calculate the missing value ration\n",
        "       and drop that variable if it's missing value ratio \n",
        "       is greater than 20% and otherwise impute or drop that\n",
        "       value\n",
        "\n",
        "    ''' \n",
        "\n",
        "    def treat_missing(self,data):#feature selection based on missing value ratio\n",
        "        cat_col = self.data.select_dtypes(exclude=['int64', 'float64']).columns #catagorical columns\n",
        "        non_cat_col = self.data.select_dtypes(include=['int64', 'float64']).columns # non catagorical variable\n",
        "    \n",
        "        data[cat_col] = data[cat_col].replace(['unknown', 0], np.nan)\n",
        "        missing_value = data.isnull().sum()/len(data)*100\n",
        "        missing_var = []\n",
        "        for i,col in enumerate(self.data.columns):\n",
        "            if missing_value[i]>=20:   #setting the threshold as 20%\n",
        "                missing_var.append(col)\n",
        "                data.drop(col, axis=1,inplace=True)\n",
        "        print(\"\\n {} variables have dropped based on missing value ratio\".format(missing_var))\n",
        "          \n",
        "        '''\n",
        "            since the rest variable's missing value is \n",
        "            smaller we can impute them\n",
        "        '''\n",
        "        print(\"\\n {} missing values have been dropped\".format(data.isnull().sum().sum()))\n",
        "        data=data.fillna(data.mode().iloc[0])\n",
        "        \n",
        "        self.data = data\n",
        "        return data\n",
        "    '''\n",
        "        we may need to drop a columns which have a less variance and\n",
        "        more correlated variables, since if they are more correlated \n",
        "        they have same impact in indecating something and if they have least\n",
        "        variance which means they are almost constant values.\n",
        "    '''\n",
        "    def drope(self,data,col=None): # which is ongoing function,not implemented in optimized way yet.\n",
        "        data.drop(col,axis =1,inplace=True)\n",
        "        self.cat_col = data.select_dtypes(exclude=['int64', 'float64']).columns #catagorical columns\n",
        "        self.non_cat = data.select_dtypes(include=['int64', 'float64']).columns #catagorical columns\n",
        "        print(\"\\n {} variable has been dropped based on high correlation and less variance analysis\".format([col]))\n",
        "        self.data = data\n",
        "        return data\n",
        "\n",
        "    def deal_outliers(self,data,columns):#handle outliers based on interquartile range(iqr)\n",
        "        orig_data = data\n",
        "\n",
        "        print(\"\\n before dealing with the ouliers the shape of the data\",\n",
        "              data.shape)\n",
        "        for col in columns:\n",
        "            q1 = data[col].quantile(0.25) # quartile 1\n",
        "            q3 = data[col].quantile(0.75) # quartile 4\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 -(1.5 * iqr) #lower whishker\n",
        "            upper_bound = q3 +(1.5 * iqr) #upper whishker\n",
        "            data = data[data[col]>lower_bound] #remove which are greater than upper whishker\n",
        "            data = data[data[col]<upper_bound] #remove which are less than lower whishker\n",
        "\n",
        "        print(\"after removing the outliers\",data.shape)\n",
        "        \n",
        "        self.data = data\n",
        "        self.orig_data = orig_data\n",
        "        \n",
        "        print(\"{}  data has been removed based on outlier analysis\".format(len(self.orig_data)-len(self.data)))\n",
        "        return data # return data with outlier and without outlier\n",
        "    \n",
        "    def encoder(self,data):# encode the catagorical value\n",
        "        label_encoder = LabelEncoder()\n",
        "        binary_col = []\n",
        "        for col in (data.columns):\n",
        "            if data[col].nunique() == 2:\n",
        "                binary_col.append(col)\n",
        "                data[col] = label_encoder.fit_transform(data[col])# encode binary variables\n",
        "        print(\"\\n{} variables have been encoded based on label encoding the rest encoded as dummy variable\".format(binary_col))\n",
        "        \n",
        "        data = pd.get_dummies(data) #encode not binary variables\n",
        "        self.data = data\n",
        "        return data\n",
        "    \n",
        "    def scaler(self,data, scaler= MinMaxScaler()): # not implmented in optimized way yet\n",
        "        data[:] = scaler.fit_transform(data[:]) #scaleing the given data\n",
        "        self.data = data\n",
        "        print(\"\\n---- Scaling the data based on\",scaler)\n",
        "        return data\n",
        "    \n",
        "    def reduce_dimension(self,data,reduction_model = 'tsne',dim=3): # reduce the dimentions of a given data\n",
        "        target = self.target\n",
        "        data_1 = data\n",
        "        d = data_1.drop(target,axis=1)\n",
        "        \n",
        "        print(\"\\nThe data is reduced based on {0} algorithm to {1} dimentions\".format(reduction_model,dim+1))\n",
        "        print(\"Executing the reduction starts and wait ......\")\n",
        "        \n",
        "        if reduction_model =='pca': # using PCA\n",
        "            pca = PCA(n_components=dim)\n",
        "            reduced_df = pd.DataFrame(pca.fit_transform(d), # reduce the dimention and convert to data frame\n",
        "               columns=[f'pca {i}'  for i in range(1,dim+1)]) \n",
        "            print ('PCA done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "            \n",
        "        elif reduction_model =='tsne':\n",
        "            tsne = TSNE(n_components = dim, n_iter = 300)\n",
        "            reduced_df =pd.DataFrame(tsne.fit_transform(d),\n",
        "                        columns=[f'tsne {i}'  for i in range(1,dim+1)])\n",
        "            print ('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "    #     else:\n",
        "    #         input_dim = d.shape[1]\n",
        "    #         input_layer = Input(shape = (input_dim,))\n",
        "    #         encoder_layer_1 = Dense(40,activation='tanh',\n",
        "    #                         activity_regularizer=regularizers.l1(10e-5)(input))(input_layer)\n",
        "    #         encoder_layer_2 = Dense(30,activation='tanh')(encoder_layer_1)\n",
        "    #         encoder_layer_3 = Dense(encoding_dim,activation='tanh')(encoder_layer_2)\n",
        "    #         #create encoder model\n",
        "    #         encoder = Model(inputs = input_layer, outputs = encoder_layer_3)\n",
        "    #         reduced_df= pd.DataFrame(encoder.predict(d))\n",
        "                   \n",
        "    # reset the index to get clean data(without nan)\n",
        "                \n",
        "        \n",
        "        data_1[target].reset_index(drop=True, inplace=True)\n",
        "        reduced_df.reset_index(drop=True, inplace=True)\n",
        "        final_df = pd.concat([reduced_df,data_1[target]],axis=1)\n",
        "                   \n",
        "        self.data = final_df\n",
        "        return final_df\n",
        "    \n",
        "    def split_data(self,data,target):  #Split the data\n",
        "        \n",
        "        print(\"\\nspliting the data as train and test\")\n",
        "        X=data.drop(target,axis=1)\n",
        "        y=data[target]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "                   \n",
        "    def pipe_and_filter(self):# this function is to pipe earlier steps\n",
        "        new = self.treat_missing(self.data)\n",
        "        new = self.drope(self.data,col= self.drop_col)\n",
        "        new = self.deal_outliers(self.data,columns = self.outlier_col)\n",
        "        new = self.encoder(self.data)\n",
        "        new_scaled = self.scaler(self.data)\n",
        "        new = self.reduce_dimension(self.data,reduction_model = self.reduction_model,dim=self.dim)\n",
        "        X_train, X_test, y_train, y_test =self.split_data(self.data,self.target)\n",
        "        print(\"DONE!\")\n",
        "        \n",
        "        result = [X_train, X_test, y_train, y_test]\n",
        "        df =[new_scaled, new]\n",
        "                   \n",
        "        return df,result\n",
        "        \n",
        "        \n",
        "            \n",
        "        \n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}